Union-Find Data Structure:

Krushkal's algorithm: To find min cost spanning tree.
  Process: Start from the shortest edge and pick the next shortest edge from the remaining edges (might be discontinuous)
  
Problem statement: I have a set S in which each element belongs to some partition (or component).
  I want to determine: given an element s, which component does it belong to? [Find operation]
                     & merging 2 components [Union operation]
                     
Krushkal's: Naive implementation - maintain an array of elements containing components, Store the edges being added.

In naive implementation --> Each Union operation takes O(n) times [n is number of elements].
	So m Union operations take O(mn) time.
	
Better implementation of Krushkal's algo:
	In addition to maintaining an array of elements containing components & storing selected edges, also store:
		a. The elements in each component
		b. Size of each component [because in some PL - this might take a lot of time]
	
In Union operation: store an array of elements containing components & also for each component - store which elements it contains
	Which merging, update only those elements in array which are present in the component to be merged.
	For each element s, size of component[s] at least doubles each time it is relabelled.
	After a sequence of m Union() operations, at most 2m elements are touched & at most m elements have had their components updated.
	Worst case: Size of component[s] grows as 1,2,4...,m in m Union operations. So, s is relabelled at most O(log(m)) times.
	So, after m Union operations, at most m elements have had their components updated, each at most O(log(m)) times. So, overall O(m*log(m))
  
Cont.... Krushkal's algo: Initially sorting the edges will take O(m log(m)) time. m <= n^2. So, O(m log(n)).
                n Union operations will take amortized cost O(n*log(n)) time. [n - elements, m - edges]
                So, overall time complexity is O((m+n) log(n))

A pointer stores an address (So, a pointer always points to ...)

**********
Week5 module 2:
Union using Pointers:
	maintain an array of elements (named 'Node') containing address where that elements' parent is stored (i.e. each array value points to the value of the parent)
	Except for the root node --> the array value points to the 'name' (or index) itself. [Looks incorrect]
	
	maintain an array of elements (named 'Node') containing address where that elements' parent's address is stored (i.e. each array value is a pointer to 
	a pointer OR each array value points to the pointer of the parent). Except for the root node --> where the pointer points to itself.
	
Find using Pointers:
	takes log(n) time for the first time. Then, we can directly point the array value to the value of root node. Then, it takes O(1) for each subsequent visit
	to that node. This is called Path Compression (or Flattening the .
	
For Krushkal's algo:
	Also keep an array to store size[j] of the tree starting at node 'j'

**************
Week 5 Module 3:

Priority queue: have 2 basic operations:
	1. delete max priority job
	2. insert a new job
	
	By using linear structures: processing a sequence of n jobs takes O(n^2) time

To get better time, maintain a special kind of binary tree called a 'heap'

**************

Week 5 module 4:
Heaps:

Should satisfy a local property: the parent should be greater than the 2 children
Insert - takes time O(log n) - equal to size of heap. Same for deleting max element & rearranging the heap.

Child of H[j] are @ H[2j+1], H[2j+2]
Parent of H[j] are @ floor(H[(j-1)/2])

Building a heap: Naive: Insert one element at a time & make sure it satisfies the heap property - O(n log(n))
	Better way: First arrange all elements randomly in form of a heap - will not satisfy the property. Now start from bottom to satisfy heap property.
		Takes O(n) time

Priority queues are implemented using heaps.

**************
Week 5 module 5:
Heaps: Updating values, sorting

In Dijkstra's algo, we can maintain a min-heap of distances. In this, we need to update distances continuously
	Maintain 2 arrays: NodeToHeap & HeapToNode to keep track of which node is where in the heap & vice-versa.
	
Heap sort: Heapify: O(n)
	Then, do delete max n times: O(n log(n)). Can sort in-place
	
**************
Week 5 module 6
Counting Inversions

Divide and Conquer: To find how close are the ranked lists [Ex. 1 2 3 4 5 vs 2 4 3 1 5] i.e. I ranked the movie A as 1st and my friend as 2nd
	Use 'Sort and count' inversions on L & R.
	Then, count inversions across L & R while merging: 'Merge and count' [If both L & R are sorted in ascending order, if one element in R needs to be selected
	that implies all other remaining elements in L are bigger than R OR their are no elements left in L]
	It takes O(n log(n)). Though total inversions can be n(n-1)/2, we are counting them efficiently w/o enumerating each one.
	
**************
Week 5 module 7
Closest pair of points:

Brute force: try every pair of points - O(n^2)
Better method: Divide the set of points (Px, Py) into (Qx, Qy) & (Rx, Ry) by drawing a vertical line in between
	Find min distance in Q & R sets and use it to find closest distance b/w points across the separator within distance d from the separator.
	
	Time: Initial time to create x-sorted (Px, Py) is O(n log(n))
	Overall T = O(n log(n))
	
**************
Week 6 module 1
Binary Search Trees:

Why are they required?: Because heaps take O(n) time to find pred & succ of any given number. This can be reduced to O(log n) using BST

BST: For each node w/ value v, values in left subtree < v & right subtree > v. There should be no duplicate values.
	Implemented using pointers - Each node has 1 value & 3 pointers (1 pointing to parent, 1 to left child and 3rd to Right child)
		We can also store height at each node.
	Can get sorted list quickly by traversing from Bottom Left to Right
	All operations (i.e. Insert, delete max, etc.) are O(log n)
	
**************
Week 6 module 2
Balanced Search Trees:

Positive slope: height of left > ht of right
In case of unbalanced trees - we do rotations
If the slope at a node 'x' is +2, and slope at TL is {0,+1} - rotate right@x

**************
Week 6 module 4
Greedy algo: Minimizing Lateness:

Ex. suppose we need to allocate n jobs to a resource and all jobs have time t(i) and deadline d(i). All jobs are to be done and we need to minimize lateness.
	Choose the job with the earliest deadline.
	
**************
Week 6 module 5
Huffman codes:

ABL - should be least for the optimal encoding
ABL - Average Bits per Letter

Leaves at max depth occur in pairs. These leaves have the lowest frequencies.
No node will have one child, otherwise that child can be promoted.
Huffman's algorithm - find a recursive solution

Time complexity comes due to finding the minimum - use heaps to reduce time complexity to k*log k times

It is a greedy algo.

**************
